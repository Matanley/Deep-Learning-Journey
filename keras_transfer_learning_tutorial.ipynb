{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The code below modified from https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "Copyright belong to the great Francois Chollet. You are strongly encouraged to make a reference to this link in understanding how to set up the data directory and the working mechanism. However, due to the Keras updates, some of the codes no longer works and I manually coded all the codes and modified some places to make it run smoothly under python 3.5 and Keras 2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code demonstrate how you can leverage Keras' preprocessing module to process image data augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# image preprocessing with keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# the later 3 methods depends on PIL, which needs to be available\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "#print(x.shape)\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "\n",
    "i = 0\n",
    "# datagen.flow(x) x has to be in rank 4\n",
    "# due to this img-array transformation\n",
    "# datagen.flow_from_directory is more often used\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                                      save_to_dir='preview', save_prefix='cat', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build a Conv2D model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "# data and superparameters setup\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# a simple way to check the data format\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "# build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# the model so far outputs 3D feature maps (height, width, features)\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#Dropout helps reduce overfitting by preventing a layer from seeing twice the exact same pattern\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 800 images belonging to 2 classes.\n",
      "Epoch 1/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6940 - acc: 0.5247Epoch 00000: val_loss improved from inf to 0.66325, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 90s - loss: 0.6939 - acc: 0.5250 - val_loss: 0.6632 - val_acc: 0.5925\n",
      "Epoch 2/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6656 - acc: 0.5958Epoch 00001: val_loss improved from 0.66325 to 0.65789, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 90s - loss: 0.6657 - acc: 0.5955 - val_loss: 0.6579 - val_acc: 0.5725\n",
      "Epoch 3/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6567 - acc: 0.6003Epoch 00002: val_loss improved from 0.65789 to 0.63053, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 89s - loss: 0.6562 - acc: 0.6015 - val_loss: 0.6305 - val_acc: 0.6813\n",
      "Epoch 4/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6411 - acc: 0.6381Epoch 00003: val_loss improved from 0.63053 to 0.61329, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 90s - loss: 0.6403 - acc: 0.6385 - val_loss: 0.6133 - val_acc: 0.6388\n",
      "Epoch 5/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6126 - acc: 0.6734Epoch 00004: val_loss improved from 0.61329 to 0.60198, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 91s - loss: 0.6120 - acc: 0.6735 - val_loss: 0.6020 - val_acc: 0.6650\n",
      "Epoch 6/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5948 - acc: 0.6830Epoch 00005: val_loss improved from 0.60198 to 0.58429, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 95s - loss: 0.5946 - acc: 0.6820 - val_loss: 0.5843 - val_acc: 0.6650\n",
      "Epoch 7/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.6956Epoch 00006: val_loss improved from 0.58429 to 0.56681, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 92s - loss: 0.5870 - acc: 0.6955 - val_loss: 0.5668 - val_acc: 0.6987\n",
      "Epoch 8/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5688 - acc: 0.7102Epoch 00007: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.5684 - acc: 0.7100 - val_loss: 0.6004 - val_acc: 0.6813\n",
      "Epoch 9/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5465 - acc: 0.7203Epoch 00008: val_loss improved from 0.56681 to 0.53793, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 90s - loss: 0.5473 - acc: 0.7200 - val_loss: 0.5379 - val_acc: 0.7400\n",
      "Epoch 10/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5575 - acc: 0.7188Epoch 00009: val_loss did not improve\n",
      "125/125 [==============================] - 91s - loss: 0.5590 - acc: 0.7185 - val_loss: 0.5584 - val_acc: 0.7225\n",
      "Epoch 11/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5350 - acc: 0.7273Epoch 00010: val_loss improved from 0.53793 to 0.51988, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 93s - loss: 0.5354 - acc: 0.7265 - val_loss: 0.5199 - val_acc: 0.7462\n",
      "Epoch 12/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.7535Epoch 00011: val_loss did not improve\n",
      "125/125 [==============================] - 91s - loss: 0.5044 - acc: 0.7525 - val_loss: 0.5995 - val_acc: 0.7013\n",
      "Epoch 13/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4940 - acc: 0.7576Epoch 00012: val_loss improved from 0.51988 to 0.48426, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 91s - loss: 0.4937 - acc: 0.7580 - val_loss: 0.4843 - val_acc: 0.7762\n",
      "Epoch 14/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5033 - acc: 0.7535Epoch 00013: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.5032 - acc: 0.7540 - val_loss: 0.5394 - val_acc: 0.7338\n",
      "Epoch 15/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4757 - acc: 0.7762Epoch 00014: val_loss did not improve\n",
      "125/125 [==============================] - 92s - loss: 0.4755 - acc: 0.7765 - val_loss: 0.5244 - val_acc: 0.7538\n",
      "Epoch 16/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4644 - acc: 0.7923Epoch 00015: val_loss did not improve\n",
      "125/125 [==============================] - 92s - loss: 0.4656 - acc: 0.7905 - val_loss: 0.5287 - val_acc: 0.7325\n",
      "Epoch 17/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4645 - acc: 0.7893Epoch 00016: val_loss did not improve\n",
      "125/125 [==============================] - 91s - loss: 0.4637 - acc: 0.7900 - val_loss: 0.4859 - val_acc: 0.7612\n",
      "Epoch 18/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4381 - acc: 0.7949Epoch 00017: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.4367 - acc: 0.7960 - val_loss: 0.5256 - val_acc: 0.7438\n",
      "Epoch 19/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4354 - acc: 0.7954Epoch 00018: val_loss improved from 0.48426 to 0.47476, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 95s - loss: 0.4371 - acc: 0.7955 - val_loss: 0.4748 - val_acc: 0.8050\n",
      "Epoch 20/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4312 - acc: 0.8075Epoch 00019: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.4319 - acc: 0.8060 - val_loss: 0.5162 - val_acc: 0.7600\n",
      "Epoch 21/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4043 - acc: 0.8216Epoch 00020: val_loss did not improve\n",
      "125/125 [==============================] - 99s - loss: 0.4064 - acc: 0.8195 - val_loss: 0.5092 - val_acc: 0.7812\n",
      "Epoch 22/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3866 - acc: 0.8221Epoch 00021: val_loss improved from 0.47476 to 0.44730, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 99s - loss: 0.3864 - acc: 0.8220 - val_loss: 0.4473 - val_acc: 0.8000\n",
      "Epoch 23/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3863 - acc: 0.8332Epoch 00022: val_loss did not improve\n",
      "125/125 [==============================] - 96s - loss: 0.3879 - acc: 0.8325 - val_loss: 0.5397 - val_acc: 0.7375\n",
      "Epoch 24/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3762 - acc: 0.8367Epoch 00023: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.3742 - acc: 0.8380 - val_loss: 0.4757 - val_acc: 0.8100\n",
      "Epoch 25/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.8342Epoch 00024: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.3642 - acc: 0.8335 - val_loss: 0.4870 - val_acc: 0.7850\n",
      "Epoch 26/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3407 - acc: 0.8533Epoch 00025: val_loss did not improve\n",
      "125/125 [==============================] - 98s - loss: 0.3402 - acc: 0.8540 - val_loss: 0.5171 - val_acc: 0.7688\n",
      "Epoch 27/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3556 - acc: 0.8407Epoch 00026: val_loss did not improve\n",
      "125/125 [==============================] - 96s - loss: 0.3552 - acc: 0.8415 - val_loss: 0.4874 - val_acc: 0.7887\n",
      "Epoch 28/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3352 - acc: 0.8468Epoch 00027: val_loss did not improve\n",
      "125/125 [==============================] - 102s - loss: 0.3343 - acc: 0.8475 - val_loss: 0.5378 - val_acc: 0.7875\n",
      "Epoch 29/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3450 - acc: 0.8453Epoch 00028: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.3454 - acc: 0.8450 - val_loss: 0.5084 - val_acc: 0.7975\n",
      "Epoch 30/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3459 - acc: 0.8458Epoch 00029: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.3452 - acc: 0.8465 - val_loss: 0.5101 - val_acc: 0.7812\n",
      "Epoch 31/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3126 - acc: 0.8720Epoch 00030: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.3120 - acc: 0.8720 - val_loss: 0.5103 - val_acc: 0.8000\n",
      "Epoch 32/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2988 - acc: 0.8669Epoch 00031: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.3007 - acc: 0.8665 - val_loss: 0.5476 - val_acc: 0.7900\n",
      "Epoch 33/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.3182 - acc: 0.8543Epoch 00032: val_loss improved from 0.44730 to 0.43986, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "125/125 [==============================] - 95s - loss: 0.3180 - acc: 0.8540 - val_loss: 0.4399 - val_acc: 0.8087\n",
      "Epoch 34/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2829 - acc: 0.8810Epoch 00033: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.2849 - acc: 0.8795 - val_loss: 0.5667 - val_acc: 0.7875\n",
      "Epoch 35/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2987 - acc: 0.8755Epoch 00034: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.2985 - acc: 0.8750 - val_loss: 0.5074 - val_acc: 0.7925\n",
      "Epoch 36/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2809 - acc: 0.8700Epoch 00035: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.2806 - acc: 0.8695 - val_loss: 0.5611 - val_acc: 0.8025\n",
      "Epoch 37/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2665 - acc: 0.8816Epoch 00036: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.2669 - acc: 0.8810 - val_loss: 0.5071 - val_acc: 0.8037\n",
      "Epoch 38/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2577 - acc: 0.8952Epoch 00037: val_loss did not improve\n",
      "125/125 [==============================] - 98s - loss: 0.2577 - acc: 0.8955 - val_loss: 0.5926 - val_acc: 0.7987\n",
      "Epoch 39/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.8795Epoch 00038: val_loss did not improve\n",
      "125/125 [==============================] - 96s - loss: 0.2605 - acc: 0.8800 - val_loss: 0.5143 - val_acc: 0.7987\n",
      "Epoch 40/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2617 - acc: 0.8851Epoch 00039: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.2632 - acc: 0.8840 - val_loss: 0.6118 - val_acc: 0.7750\n",
      "Epoch 41/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2469 - acc: 0.8962Epoch 00040: val_loss did not improve\n",
      "125/125 [==============================] - 94s - loss: 0.2469 - acc: 0.8965 - val_loss: 0.5422 - val_acc: 0.8113\n",
      "Epoch 42/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2466 - acc: 0.8992Epoch 00041: val_loss did not improve\n",
      "125/125 [==============================] - 94s - loss: 0.2471 - acc: 0.8990 - val_loss: 0.5562 - val_acc: 0.8050\n",
      "Epoch 43/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9027Epoch 00042: val_loss did not improve\n",
      "125/125 [==============================] - 98s - loss: 0.2357 - acc: 0.9035 - val_loss: 0.6013 - val_acc: 0.8113\n",
      "Epoch 44/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2328 - acc: 0.8992Epoch 00043: val_loss did not improve\n",
      "125/125 [==============================] - 98s - loss: 0.2346 - acc: 0.8980 - val_loss: 0.5655 - val_acc: 0.7950\n",
      "Epoch 45/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2248 - acc: 0.9093Epoch 00044: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.2240 - acc: 0.9095 - val_loss: 0.8221 - val_acc: 0.7750\n",
      "Epoch 46/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9007Epoch 00045: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.2331 - acc: 0.9005 - val_loss: 0.5601 - val_acc: 0.8175\n",
      "Epoch 47/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2301 - acc: 0.8987Epoch 00046: val_loss did not improve\n",
      "125/125 [==============================] - 97s - loss: 0.2297 - acc: 0.8990 - val_loss: 0.5740 - val_acc: 0.8150\n",
      "Epoch 48/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2251 - acc: 0.9078Epoch 00047: val_loss did not improve\n",
      "125/125 [==============================] - 95s - loss: 0.2266 - acc: 0.9075 - val_loss: 0.5190 - val_acc: 0.7950\n",
      "Epoch 49/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.1870 - acc: 0.9194Epoch 00048: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.1871 - acc: 0.9195 - val_loss: 0.5782 - val_acc: 0.8337\n",
      "Epoch 50/50\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.2058 - acc: 0.9183Epoch 00049: val_loss did not improve\n",
      "125/125 [==============================] - 93s - loss: 0.2050 - acc: 0.9190 - val_loss: 0.5931 - val_acc: 0.8175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1475044eb00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# setup a checkpoint to save only the best weights\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "# below is the data augmentation configuration used for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# below is the data augmentation configuration used for testing, with only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# data generator which process the img in parallel with the training or testing\n",
    "# flow_from_directory() will generate an infinite number of batches\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary')\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=nb_train_samples // batch_size, \n",
    "    epochs=epochs, \n",
    "    callbacks=[checkpointer],\n",
    "    validation_data=validation_generator, \n",
    "    validation_steps=nb_validation_samples // batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use bottleneck features from VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below demonstrate how to use bottleneck features generated from VGG16 imageNet weights. To get the bottleneck feature, you need to first pass all the training data through the VGG16 architecture, with all the weights freezed to get the feature maps before the former VGG16 FC layers, then use these bottleneck features as inputs to build a small model and train the ontop layer parameters.\n",
    "\n",
    "Note if you are using bottleneck features, you should not use data augumentation techniques anymore and only rescaling is allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 800 samples\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.4698 - acc: 0.7965 - val_loss: 0.2886 - val_acc: 0.8762\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 7s - loss: 0.2728 - acc: 0.8835 - val_loss: 0.3415 - val_acc: 0.8175\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 7s - loss: 0.2112 - acc: 0.9190 - val_loss: 0.2428 - val_acc: 0.8988\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 7s - loss: 0.1866 - acc: 0.9280 - val_loss: 0.2307 - val_acc: 0.9137\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 7s - loss: 0.1545 - acc: 0.9390 - val_loss: 0.2468 - val_acc: 0.8988\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.1328 - acc: 0.9495 - val_loss: 0.2555 - val_acc: 0.9075\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.1144 - acc: 0.9520 - val_loss: 0.2780 - val_acc: 0.8975\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0977 - acc: 0.9640 - val_loss: 0.2951 - val_acc: 0.8962\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0611 - acc: 0.9780 - val_loss: 0.3119 - val_acc: 0.8938\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0619 - acc: 0.9765 - val_loss: 0.3175 - val_acc: 0.9038\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0482 - acc: 0.9800 - val_loss: 0.2884 - val_acc: 0.9062\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0530 - acc: 0.9795 - val_loss: 0.3175 - val_acc: 0.8975\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0485 - acc: 0.9810 - val_loss: 0.3513 - val_acc: 0.9075\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0515 - acc: 0.9800 - val_loss: 0.3500 - val_acc: 0.8850\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0391 - acc: 0.9860 - val_loss: 0.3691 - val_acc: 0.8988\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0357 - acc: 0.9860 - val_loss: 0.3725 - val_acc: 0.9050\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0537 - acc: 0.9820 - val_loss: 0.3339 - val_acc: 0.9012\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0291 - acc: 0.9900 - val_loss: 0.3484 - val_acc: 0.9038\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0295 - acc: 0.9875 - val_loss: 0.4541 - val_acc: 0.8788\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0362 - acc: 0.9870 - val_loss: 0.4075 - val_acc: 0.9075\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0242 - acc: 0.9910 - val_loss: 0.5277 - val_acc: 0.8712\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0233 - acc: 0.9915 - val_loss: 0.4725 - val_acc: 0.8925\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0190 - acc: 0.9945 - val_loss: 0.4328 - val_acc: 0.8988\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0179 - acc: 0.9920 - val_loss: 0.3898 - val_acc: 0.9075\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0230 - acc: 0.9920 - val_loss: 0.4496 - val_acc: 0.8975\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0219 - acc: 0.9935 - val_loss: 0.4212 - val_acc: 0.8950\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0492 - acc: 0.9845 - val_loss: 0.6728 - val_acc: 0.8588\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0813 - acc: 0.9705 - val_loss: 0.4288 - val_acc: 0.8962\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 8s - loss: 0.0344 - acc: 0.9870 - val_loss: 0.4081 - val_acc: 0.9038\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0245 - acc: 0.9910 - val_loss: 0.4232 - val_acc: 0.9125\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0353 - acc: 0.9875 - val_loss: 0.3820 - val_acc: 0.9050\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0243 - acc: 0.9925 - val_loss: 0.5808 - val_acc: 0.8825\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0314 - acc: 0.9890 - val_loss: 0.4122 - val_acc: 0.9062\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0328 - acc: 0.9875 - val_loss: 0.5447 - val_acc: 0.8900\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0301 - acc: 0.9885 - val_loss: 0.4693 - val_acc: 0.8900\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 10s - loss: 0.0114 - acc: 0.9960 - val_loss: 0.5256 - val_acc: 0.9038\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0077 - acc: 0.9980 - val_loss: 0.5486 - val_acc: 0.9012\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 10s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.5511 - val_acc: 0.9100\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0078 - acc: 0.9980 - val_loss: 0.8106 - val_acc: 0.8788\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0266 - acc: 0.9855 - val_loss: 0.5098 - val_acc: 0.9125\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0304 - acc: 0.9880 - val_loss: 0.6583 - val_acc: 0.8838\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0335 - acc: 0.9880 - val_loss: 0.4986 - val_acc: 0.8912\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0214 - acc: 0.9925 - val_loss: 0.5159 - val_acc: 0.8975\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0178 - acc: 0.9930 - val_loss: 0.4825 - val_acc: 0.9025\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0236 - acc: 0.9915 - val_loss: 0.6191 - val_acc: 0.9038\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0234 - acc: 0.9905 - val_loss: 0.6061 - val_acc: 0.8938\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0233 - acc: 0.9935 - val_loss: 0.6042 - val_acc: 0.8838\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0179 - acc: 0.9940 - val_loss: 0.5391 - val_acc: 0.9075\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.6263 - val_acc: 0.8938\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 9s - loss: 0.0222 - acc: 0.9895 - val_loss: 0.6766 - val_acc: 0.8912\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "def save_bottleneck_features():\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    # import the VGG16 network with FC layers removed\n",
    "    # here weights are generated from imagenet dataset\n",
    "    model = VGG16(include_top=False, weights='imagenet')\n",
    "    \n",
    "    # training set bottleneck features generating\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir, \n",
    "        target_size=(img_width, img_height), \n",
    "        batch_size=batch_size, \n",
    "        class_mode=None, \n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_train = model.predict_generator(\n",
    "        train_generator, \n",
    "        nb_train_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_train.npy', 'wb'), bottleneck_features_train)\n",
    "    \n",
    "    # validation set bottleneck features generating\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir, \n",
    "        target_size=(img_width, img_height), \n",
    "        batch_size=batch_size, \n",
    "        class_mode=None, \n",
    "        shuffle=False)\n",
    "    \n",
    "    bottleneck_features_validation = model.predict_generator(\n",
    "        validation_generator, \n",
    "        nb_validation_samples // batch_size)\n",
    "    \n",
    "    np.save(open('bottleneck_features_validation.npy', 'wb'), bottleneck_features_validation)\n",
    "    \n",
    "def train_top_model():\n",
    "    train_data = np.load(open('bottleneck_features_train.npy', 'rb'))\n",
    "    train_labels = np.array(\n",
    "        [0]*int(nb_train_samples / 2) + [1]*int(nb_train_samples / 2))\n",
    "    \n",
    "    validation_data = np.load(open('bottleneck_features_validation.npy', 'rb'))\n",
    "    validation_labels = np.array(\n",
    "        [0]*int(nb_validation_samples / 2) + [1]*int(nb_validation_samples / 2))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    \n",
    "save_bottleneck_features()\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fine tuning top layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Below is a demonstration of fine tuning the last few layers of a trained model, some instructions from Francois are:\n",
    "\n",
    "- in order to perform fine-tuning, all layers should start with properly trained weights: for instance you should not slap a randomly initialized fully-connected network on top of a pre-trained convolutional base. This is because the large gradient updates triggered by the randomly initialized weights would wreck the learned weights in the convolutional base. In our case this is why we first train the top-level classifier, and only then start fine-tuning convolutional weights alongside it (This means the fine tuning must come after the bottleneck implementation)\n",
    "- we choose to only fine-tune the last convolutional block rather than the entire network in order to prevent overfitting, since the entire network would have a very large entropic capacity and thus a strong tendency to overfit. The features learned by low-level convolutional blocks are more general, less abstract than those found higher-up, so it is sensible to keep the first few blocks fixed (more general features) and only fine-tune the last one (more specialized features)\n",
    "- fine-tuning should be done with a very slow learning rate, and typically with the SGD optimizer rather than an adaptative learning rate optimizer such as RMSProp. This is to make sure that the magnitude of the updates stays very small, so as not to wreck the previously learned features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# below weights actually come from the second example with bottleneck features\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "\n",
    "\n",
    "#dimensions for our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data/train'\n",
    "validation_data_dir = 'data/validation'\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "# the input_shape has to be added otherwise it will pop out an error\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
    "print('Model loaded')\n",
    "\n",
    "# build a classifier model to put on top of the convolutional model\n",
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=model.output_shape[1:]))\n",
    "top_model.add(Dense(256, activation='relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# note that it is necessary to start with a fully-trained\n",
    "# classifier, including the top classifier,\n",
    "# in order to successfully do fine-tuning\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# Model build has to be done this way other than add in Keras 2.x\n",
    "model = Model(inputs=model.input, outputs=top_model(model.output))\n",
    "\n",
    "# set the first 15 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "# here the author use 25, which is a typo\n",
    "for layer in model.layers[:15]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compile the model with SGD/momentum optimizer\n",
    "# and a very slow learning rate\n",
    "model.compile(loss='binary_crossentropy', \n",
    "                      optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255, \n",
    "    shear_range=0.2, \n",
    "    zoom_range=0.2, \n",
    "    horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir, \n",
    "    target_size=(img_height, img_width), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_data_dir, \n",
    "    target_size=(img_height, img_width), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=nb_train_samples, \n",
    "    epochs=epochs, \n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples)\n",
    "\n",
    "model.save_weights('fine_tuning_VGG16.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "dog-project",
   "language": "python",
   "name": "dog-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

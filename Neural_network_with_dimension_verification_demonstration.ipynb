{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I put up three examples to demonstrate both the forward and backward propagation process of simple neural network.The first one is using input with a rank 1 object or when you use zip(x, y) to iterate through the samples. The problem with rank 1 array is when you do the calculation, the dimension of the operation become subtle to track which may end up with bugs you do not know. This is the reason why Andrew NG specifically addressed that you should not use rank 1 array during modeling. \n",
    "\n",
    "Hence the second example is an improvement by changing any rank 1 array to rank 2 array.\n",
    "\n",
    "The third example goes with vectorized sample training which is what really happens in real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.10804047 -0.29444082]\n",
      "[[ 0.50017701 -0.60051118]\n",
      " [ 0.1000354  -0.20010224]\n",
      " [ 0.0999292   0.70020447]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Use Sigmoid as the activation function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2]) # here x is a rank 1 object\n",
    "y = 0.6\n",
    "\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                                 [0.1, -0.2],\n",
    "                                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3]) # shape 2,\n",
    "\n",
    "# Forward propagation\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input) # shape 2, \n",
    "#print(hidden_layer_output)\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_input)\n",
    "#print(output) # a number\n",
    "\n",
    "# Backwards propagation\n",
    "\n",
    "# Calculate output error\n",
    "error = y - output # a number, rank 0\n",
    "\n",
    "\n",
    "# Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output) # a number\n",
    "#print(output_error_term)\n",
    "\n",
    "# Calculate error term for hidden layer\n",
    "hidden_error_term = output_error_term *weights_hidden_output *\\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output) # shape 2,\n",
    "#print(hidden_error_term)\n",
    "\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "weights_hidden_output += learnrate * output_error_term * hidden_layer_output # shape 2, \n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "weights_input_hidden += learnrate * hidden_error_term * x[:, None]\n",
    "\n",
    "#print('weights update for hidden layer to output layer:')\n",
    "print(weights_hidden_output)\n",
    "#print('weights update for input layer to hidden layer:')\n",
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10804047]\n",
      " [-0.29444082]]\n",
      "[[ 0.50017701 -0.60051118]\n",
      " [ 0.1000354  -0.20010224]\n",
      " [ 0.0999292   0.70020447]]\n"
     ]
    }
   ],
   "source": [
    "# single sample with no Rank 1 objects\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # Use Sigmoid as the activation function\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array([[0.5, 0.1, -0.2]]) # change shape for test\n",
    "y = np.array([[0.6]]) # change shape for test\n",
    "\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                                 [0.1, -0.2],\n",
    "                                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([[0.1], [-0.3]]) # change shape for test\n",
    "\n",
    "# Forward propagation\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden) # shape 1, 2\n",
    "hidden_layer_output = sigmoid(hidden_layer_input) # shape 1, 2\n",
    "#print(hidden_layer_output)\n",
    "\n",
    "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) \n",
    "output = sigmoid(output_layer_input) # shape 1, 1\n",
    "#print(output_layer_input.shape)\n",
    "# Backwards propagation\n",
    "\n",
    "# Calculate output error\n",
    "error = y - output # shape 1, 1\n",
    "\n",
    "# Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output) # shape 1, 1\n",
    "#print(output_error_term)\n",
    "\n",
    "# Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output.T) *\\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "#print(hidden_error_term) #shape 1, 2\n",
    "\n",
    "# Calculate change in weights for hidden layer to output layer\n",
    "#weights_hidden_output += learnrate * output_error_term * hidden_layer_output.T\n",
    "weights_hidden_output += learnrate * np.dot(hidden_layer_output.T, output_error_term)\n",
    "\n",
    "# Calculate change in weights for input layer to hidden layer\n",
    "weights_input_hidden += learnrate * np.dot(x.T, hidden_error_term)\n",
    "#weights_input_hidden += learnrate *  hidden_error_term * x.T\n",
    "\n",
    "#print('weights update for hidden layer to output layer:')\n",
    "print(weights_hidden_output)\n",
    "#print('weights update for input layer to hidden layer:')\n",
    "print(weights_input_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: \n",
      "[[-0.20143431  0.56794144  0.33539595  0.04057874]\n",
      " [ 0.4489087   0.33599404  0.84973866  0.960238  ]\n",
      " [-0.15079068 -0.39760774 -0.40121413  1.12030401]]\n",
      "[[ 1.04235695]\n",
      " [ 0.26345293]\n",
      " [-0.33186789]\n",
      " [ 0.06592214]]\n",
      "Error after 20 iterations: 0.249671646691\n",
      "Weights update after every 20 epoch: \n",
      "[[-0.31186521  0.57647642  0.21399002 -0.13309149]\n",
      " [ 0.42749989  0.33908914  0.7814787   0.96151458]\n",
      " [-0.3479972  -0.40109341 -0.480883    1.11669635]]\n",
      "[[ 0.87226058]\n",
      " [ 0.13100723]\n",
      " [-0.46314094]\n",
      " [-0.25683172]]\n",
      "Error after 40 iterations: 0.248555608688\n",
      "Weights update after every 20 epoch: \n",
      "[[-0.45821546  0.61137029  0.1644987  -0.37881868]\n",
      " [ 0.49678079  0.36928896  0.74287451  1.00909503]\n",
      " [-0.47135973 -0.36300227 -0.55002109  1.10811792]]\n",
      "[[ 0.94413674]\n",
      " [ 0.22826295]\n",
      " [-0.41109329]\n",
      " [-0.38283076]]\n",
      "Error after 60 iterations: 0.243826210867\n",
      "Weights update after every 20 epoch: \n",
      "[[-0.7921686   0.70025013  0.14881824 -0.80393464]\n",
      " [ 0.69027893  0.45415457  0.72157744  1.20239422]\n",
      " [-0.67376173 -0.26738321 -0.6111616   1.09236471]]\n",
      "[[ 1.1684149 ]\n",
      " [ 0.43940071]\n",
      " [-0.31528762]\n",
      " [-0.68466592]]\n",
      "Error after 80 iterations: 0.208572028959\n",
      "Weights update after every 20 epoch: \n",
      "[[-1.72609207  1.00284109  0.15137427 -1.62484956]\n",
      " [ 1.38439873  0.78482729  0.71511052  1.93066983]\n",
      " [-1.09006185  0.00364683 -0.66122388  1.20971745]]\n",
      "[[ 1.95281262]\n",
      " [ 0.93200691]\n",
      " [-0.16363967]\n",
      " [-1.5312133 ]]\n"
     ]
    }
   ],
   "source": [
    "#Training with multiple samples, this is the most practical way in real application\n",
    "# by which both inputs and outputs are vectorized as matrix\n",
    "import numpy as np\n",
    "\n",
    "alpha = 10\n",
    "\n",
    "# use sigmoid as the activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# This is a 2 layer neural network with 1 hidden layer, with 3 input features, 4 hidden layer units and 1 output unit.\n",
    "X = np.array([[0, 0, 1],\n",
    "            [0, 1, 1],\n",
    "            [1, 0, 1],\n",
    "            [1, 1, 1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [0]])\n",
    "\n",
    "    \n",
    "n_features = X.shape[1]\n",
    "\n",
    "\n",
    "#seeds the random module so the results are the same for debugging convenience\n",
    "np.random.seed(32)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "weights_input_hidden = np.random.normal(scale= 1 / n_features**0.5, size=(3, 4))    \n",
    "weights_hidden_output = np.random.normal(scale = 1/ n_features**0.5, size=(4, 1))\n",
    "\n",
    "print(\"Initial weights: \")\n",
    "print(weights_input_hidden)\n",
    "print(weights_hidden_output)\n",
    "\n",
    "# epoch is 100\n",
    "for j in range(100):\n",
    "\n",
    "\n",
    "    # Feed forward through hidden layer and output layer\n",
    "    hidden_layer_output = sigmoid(np.dot(X, weights_input_hidden)) # shape: 4, 4\n",
    "    \n",
    "    final_output = sigmoid(np.dot(hidden_layer_output, weights_hidden_output)) # shape: 4, 1\n",
    "\n",
    "    # the difinition of error actually determine the positive or negative addition of the weights\n",
    "    final_output_error = y - final_output # shape: 4, 1\n",
    "\n",
    "    # final output error term equals output error times the derivatives of output layer's activation fuction\n",
    "    final_output_error_term = final_output_error * final_output * (1 - final_output) # shape 4, 1\n",
    "    \n",
    "    # how much did each hidden layer value contribute to the output layer error (according to the weights)?\n",
    "    hidden_layer_output_error = final_output_error_term.dot(weights_hidden_output.T) # shape: 4, 1 * 1, 4\n",
    "    hidden_layer_output_error_term = hidden_layer_output_error * hidden_layer_output * (1 - hidden_layer_output) # shape 4, 4\n",
    "    \n",
    "    # weights update for each epoch\n",
    "    weights_hidden_output_difference = np.dot(hidden_layer_output.T, final_output_error_term) # shape 4, 1 = 4, 4 * 4, 1\n",
    "    weights_input_hidden_difference = np.dot(X.T, hidden_layer_output_error_term) # shape 3, 4 = 3, 4 * 4, 4\n",
    "    \n",
    "    # update the weights with averaged difference derived by all inputs\n",
    "    weights_hidden_output += alpha * weights_hidden_output_difference / n_features\n",
    "    weights_input_hidden += alpha * weights_input_hidden_difference / n_features\n",
    "    \n",
    "    if j > 0 and j % 20 == 0:\n",
    "        print(\"Error after \"+ str(j) +\" iterations: \" + str(np.mean(final_output_error**2)))\n",
    "        print(\"Weights update after every 20 epoch: \")\n",
    "        print(weights_input_hidden)\n",
    "        print(weights_hidden_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
